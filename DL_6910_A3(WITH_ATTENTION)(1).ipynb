{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4B1RUJSBFG54",
    "outputId": "8714f6f2-02c7-4955-d798-cfd0001593f8"
   },
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "14lSUj1gGDUG",
    "outputId": "685a37f2-8350-49ef-f165-9a54d8a1eedd"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import wandb\n",
    "wandb.login()\n",
    "#!wandb login --relogin\n",
    "#!wandb login --relogin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QDNMr5sFPpe"
   },
   "outputs": [],
   "source": [
    "sos = 0\n",
    "eos = 1\n",
    "\n",
    "# tokenization\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.letter_to_index = {}\n",
    "        self.letter_to_count = {}\n",
    "        self.index_to_letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2  \n",
    "\n",
    "    def add_letter(self, letter): # making a dictionary of letters and their counts\n",
    "        if letter not in self.letter_to_index:\n",
    "            self.letter_to_index[letter] = self.n_letters\n",
    "            self.letter_to_count[letter] = 1\n",
    "            self.index_to_letter[self.n_letters] = letter\n",
    "            self.n_letters = self.n_letters + 1\n",
    "        else:\n",
    "            self.letter_to_count[letter] = self.letter_to_count[letter] + 1\n",
    "\n",
    "    def add_word(self, letter): # adding a word to the dictionary\n",
    "        for letter in letter:\n",
    "            self.add_letter(letter)\n",
    "\n",
    "  #  def decode(self, target):\n",
    "  #    return ' 'join([self.index_to_letter[i.get] for i in target])\n",
    "\n",
    "    def decode(self, target):\n",
    "        words = []\n",
    "        for i in target:\n",
    "            words.append(self.index_to_letter[i.get])\n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjdKRrTBFdkS"
   },
   "outputs": [],
   "source": [
    "def readLang(lang1, lang2, reverse=False): # read the file and make a dictionary of words of both languages\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    train_lines = open('hin_train.csv', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    val_lines = open('hin_valid.csv', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    test_lines = open('hin_test.csv', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "   # Split every line into pairs and normalize\n",
    "    train_pairs = []\n",
    "    for l in train_lines:\n",
    "        train_pairs.append(l.split(','))\n",
    "\n",
    "    val_pairs = []\n",
    "    for l in val_lines:\n",
    "        val_pairs.append(l.split(','))\n",
    "        \n",
    "    test_pairs = []\n",
    "    for l in test_lines:\n",
    "        test_pairs.append(l.split(','))\n",
    " \n",
    "\n",
    "\n",
    "    inp_lang = Lang(lang1)\n",
    "    out_lang = Lang(lang2)\n",
    "\n",
    "    for pair in train_pairs:\n",
    "        inp_lang.add_word(pair[0])\n",
    "        out_lang.add_word(pair[1])\n",
    "    \n",
    "    for pair in val_pairs:\n",
    "        inp_lang.add_word(pair[0])\n",
    "        out_lang.add_word(pair[1])\n",
    "        \n",
    "    for pair in test_pairs:\n",
    "        inp_lang.add_word(pair[0])\n",
    "        out_lang.add_word(pair[1])\n",
    "            \n",
    "   \n",
    "\n",
    "    return train_pairs, val_pairs, test_pairs, inp_lang, out_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TSCkBNcFh_4"
   },
   "outputs": [],
   "source": [
    "def indexes_From_word(lang, word): # convert a word to a list of indexes\n",
    "    indexes_ = []\n",
    "    for letter in word:\n",
    "        indexes_.append(lang.letter_to_index[letter])\n",
    "    return indexes_\n",
    "\n",
    "\n",
    "def tensor_From_word(lang, word): # convert a word to a tensor\n",
    "    indexes = indexes_From_word(lang, word)\n",
    "    indexes.append(eos)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensors_From_Pair(pair, inp_lang, out_lang): # convert a pair of words to a pair of tensors\n",
    "    inp_tensor = tensor_From_word(inp_lang, pair[0])\n",
    "    Target_tensor = tensor_From_word(out_lang, pair[1])\n",
    "    return (inp_tensor, Target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPhDUOwcFqRg"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # encoder class\n",
    "    def __init__(self,rnn_type, inp_size, emb_size, Target_tensor, p, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.Target_tensor = Target_tensor\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(inp_size, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, Target_tensor, num_layers, dropout = p)\n",
    "        self.gru = nn.GRU(emb_size, Target_tensor, num_layers, dropout = p)\n",
    "        self.lstm = nn.LSTM(emb_size, Target_tensor, num_layers, dropout = p)\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1) #embedding of word\n",
    "        embedded = self.dropout(embedded)\n",
    "        output = embedded\n",
    "        \n",
    "        # giving output according to model type\n",
    "        if self.rnn_type == 'RNN':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def initHidden(self):  # initializing hidden layer\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (\n",
    "                torch.zeros(self.num_layers, 1, self.Target_tensor, device=device),\n",
    "                torch.zeros(self.num_layers, 1, self.Target_tensor, device=device),\n",
    "            )\n",
    "        return torch.zeros(self.num_layers, 1, self.Target_tensor, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKfIkkduFs-f"
   },
   "outputs": [],
   "source": [
    "class Att_Decoder(nn.Module): # decoder class\n",
    "    def __init__(self, rnn_type, out_size, Target_tensor, p, num_layers):\n",
    "        super(Att_Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.out_size = out_size\n",
    "        self.Target_tensor = Target_tensor\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(out_size, Target_tensor)\n",
    "        self.attn = nn.Linear(Target_tensor*2, 50)\n",
    "        self.attn_combine = nn.Linear(Target_tensor*2, Target_tensor)\n",
    "        self.rnn = nn.RNN(Target_tensor, Target_tensor, num_layers, dropout = p)\n",
    "        self.gru = nn.GRU(Target_tensor, Target_tensor, num_layers, dropout = p)\n",
    "        self.lstm = nn.LSTM(Target_tensor, Target_tensor, num_layers, dropout = p)\n",
    "        self.out = nn.Linear(Target_tensor, out_size)\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # embedding of word\n",
    "        embedded = self.dropout(embedded)\n",
    "        Att_Weight = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # attention weights\n",
    "        Att_Applied = torch.bmm(Att_Weight.unsqueeze(0), encoder_outputs.unsqueeze(0)) # attention applied\n",
    "        output = torch.cat((embedded[0], Att_Applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # giving output according to model type\n",
    "        if self.rnn_type == 'RNN':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        # softmaxfunction to get probabilities\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, Att_Weight\n",
    "    \n",
    "    def initHidden(self): # initializing hidden layer\n",
    "        if self.v == 'LSTM':\n",
    "            return (torch.zeros(self.num_layers, 1, self.Target_tensor, device=device), \n",
    "                    torch.zeros(self.num_layers, 1, self.Target_tensor, device=device))\n",
    "        return torch.zeros(self.num_layers, 1, self.Target_tensor, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAGayBRbFwnQ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ5J520LFz8K"
   },
   "outputs": [],
   "source": [
    "class Train(): # training class\n",
    "    def __init__(self, train_data, encoder, decoder, criterion, tf_ratio = 0.5):\n",
    "        self.train_data = train_data\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.criterion = criterion\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.train_pairs, self.val_pairs, self.test_pairs, self.inp_lang, self.out_lang = readLang('eng', 'hin')\n",
    "        #self.training_pairs = [tensors_From_Pair(self.train_pairs[i], self.inp_lang, self.out_lang) for i in range(len(self.train_pairs))]\n",
    "        self.training_pairs = []\n",
    "        for i in range(len(self.train_pairs)):\n",
    "            pair = self.train_pairs[i]\n",
    "            tensors = tensors_From_Pair(pair, self.inp_lang, self.out_lang)\n",
    "            self.training_pairs.append(tensors)\n",
    "\n",
    "    def train(self, inp_tensor, Target_tensor, encoder_optimizer, decoder_optimizer):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_outputs = torch.zeros(50, self.encoder.Target_tensor, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        input_length = inp_tensor.size(0)\n",
    "        target_len = Target_tensor.size(0)\n",
    "\n",
    "        for i in range(input_length): # encoding a word\n",
    "            \n",
    "            encoder_output, encoder_hidden = self.encoder(inp_tensor[i], encoder_hidden)\n",
    "            # print(encoder_output.shape)\n",
    "            # print(encoder_output.shape)\n",
    "            # print(encoder_hid.shape)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[sos]], device=device)\n",
    "        decoder_hidden = encoder_hidden # encoder shares its hidden layer with decoder_\n",
    "\n",
    "        Teacher_Forcing = True if random.random() < self.tf_ratio else False\n",
    "        \n",
    "        if Teacher_Forcing: \n",
    "            for i in range(target_len):\n",
    "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                loss += self.criterion(decoder_output, Target_tensor[i])\n",
    "                decoder_input = Target_tensor[i] # teacher _forcing\n",
    "\n",
    "        else:\n",
    "            for i in range(target_len):\n",
    "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1) # top_ k predictions\n",
    "                decoder_input = topi.squeeze().detach() # detach from history as input_\n",
    "                loss += self.criterion(decoder_output, Target_tensor[i])\n",
    "                if decoder_input.item() == eos: # if EOS token is predicted_, stop\n",
    "                    break\n",
    "\n",
    "        loss.backward() \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_len\n",
    "\n",
    "\n",
    "    def train_Iters(self, optimizer, learning_rate, n_iters = 69, print_every = 69, epochs=-1):\n",
    "        start = time.time()\n",
    "        print_loss_total = 0\n",
    "\n",
    "        if optimizer == 'SGD':\n",
    "            encoder_optimizer = optim.SGD(self.encoder.parameters(), lr = learning_rate)\n",
    "            decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = learning_rate)\n",
    "        elif optimizer == 'Adam':\n",
    "            encoder_optimizer = optim.Adam(self.encoder.parameters(), lr = learning_rate)\n",
    "            decoder_optimizer = optim.Adam(self.decoder.parameters(), lr = learning_rate)\n",
    "        elif optimizer == 'RMSprop':\n",
    "            encoder_optimizer = optim.RMSprop(self.encoder.parameters(), lr = learning_rate)\n",
    "            decoder_optimizer = optim.RMSprop(self.decoder.parameters(), lr = learning_rate)\n",
    "        elif optimizer == 'NAdam':\n",
    "            encoder_optimizer = optim.NAdam(self.encoder.parameters(), lr = learning_rate)\n",
    "            decoder_optimizer = optim.NAdam(self.decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "        if epochs != -1: # if epochs are specified\n",
    "            n_iters = len(self.train_pairs)\n",
    "        else:\n",
    "            train_loss_total = 0\n",
    "            for iter in tqdm(range(1, n_iters+1)):\n",
    "                training_pair = self.training_pairs[iter - 1]\n",
    "                inp_tensor = training_pair[0]\n",
    "                Target_tensor = training_pair[1]\n",
    "                loss = self.train(inp_tensor, Target_tensor, encoder_optimizer, decoder_optimizer)\n",
    "                train_loss_total += loss\n",
    "\n",
    "                if iter % print_every == 0:\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
    "            train_acc = self.evaluateData(self.train_pairs) #evaluating the model on train pairs_\n",
    "            valid_acc = self.evaluateData(self.val_pairs) # evaluating the model on validation pairs_\n",
    "            #test_acc = self.evaluateData(self.test_pairs)\n",
    "            return train_acc, valid_acc\n",
    "\n",
    "        Training_loss = []\n",
    "        Val_accuracy = []\n",
    "        Training_accuracy = []\n",
    "        for j in range(epochs):\n",
    "            train_loss_total = 0\n",
    "            for iter in tqdm(range(1, n_iters+1)):\n",
    "                training_pair = self.training_pairs[iter - 1]\n",
    "                inp_tensor = training_pair[0]\n",
    "                Target_tensor = training_pair[1]\n",
    "                loss = self.train(inp_tensor, Target_tensor, encoder_optimizer, decoder_optimizer)\n",
    "                train_loss_total += loss\n",
    "                print_loss_total += loss\n",
    "\n",
    "                if iter % print_every == 0:\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
    "            train_acc = self.evaluateData(self.train_pairs)\n",
    "            valid_acc = self.evaluateData(self.val_pairs)\n",
    "            Training_loss.append(train_loss_total / n_iters)\n",
    "            Val_accuracy.append(valid_acc)\n",
    "            Training_accuracy.append(train_acc)\n",
    "            print({'train_loss': train_loss_total / n_iters, 'train_acc': train_acc, 'valid_acc': valid_acc})\n",
    "            wandb.log({'train_loss': train_loss_total / n_iters, 'train_acc': train_acc, 'valid_acc': valid_acc})\n",
    "        return Training_loss, Training_accuracy, Val_accuracy\n",
    "                    \n",
    "\n",
    "    def evaluate(self, word):\n",
    "        with torch.no_grad():\n",
    "            inp_tensor = tensor_From_word(self.inp_lang, word)\n",
    "            input_length = inp_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(50, self.encoder.Target_tensor, device=device)\n",
    "\n",
    "            for i in range(input_length): # encoding_ a word\n",
    "                encoder_output, encoder_hidden = self.encoder(inp_tensor[i], encoder_hidden)\n",
    "                # print(encoder_output.shape)\n",
    "                encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[sos]], device=device)\n",
    "            decoder_hidden = encoder_hidden # encoder_ shares its hidden layer with decoder\n",
    "\n",
    "            decoded_word = ''\n",
    "            decoder_attentions = torch.zeros(50, 50)\n",
    "\n",
    "            for j in range(50):\n",
    "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_attentions[j] = decoder_attention.data\n",
    "                topv, topi = decoder_output.topk(1) # top_ k predictions\n",
    "                if topi.item() == eos:\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_word += (self.out_lang.index_to_letter[topi.item()])\n",
    "                decoder_input = topi.squeeze().detach() # detach_ from history as input\n",
    "\n",
    "            return decoded_word, decoder_attentions[:j+1]\n",
    "        \n",
    "    def evaluateData(self, data):\n",
    "        acc = 0\n",
    "        for word,target in data:\n",
    "            output_word, attentions = self.evaluate(word)\n",
    "            acc += (output_word == target)\n",
    "        return acc / len(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImlwSQkhH9nU"
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs,test_pairs, inp_lang, out_lang = readLang('eng', 'hin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlOnu_3lUpH-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWijFJXLF27-"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', \n",
    "    'metric': {\n",
    "        'name': 'valid_acc',\n",
    "        'goal': 'maximize' # goal is to maximize the validation accuracy\n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'values': ['SGD', 'Adam', 'RMSprop']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-4, 5e-4, 0.001, 0.005]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'hid_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'emb_size': {\n",
    "            'values': [64, 128, 256, 512]\n",
    "        },\n",
    "        'Target_tensor': {\n",
    "            'values': [64, 128, 256, 512]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.1, 0.2, 0.3]\n",
    "        },\n",
    "        'type_t': {\n",
    "            'values': ['RNN', 'LSTM', 'GRU']\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "57hYVxrsF6KR",
    "outputId": "b7df1411-070b-41d1-9605-28ec8c5da7cf"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Default values for hyper-parameters\n",
    "    config_defaults = {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.005,\n",
    "        'epochs': 10,\n",
    "        'hid_layers': 1,\n",
    "        'emb_size': 256,\n",
    "        'Target_tensor': 256,\n",
    "        'dropout': 0.1,\n",
    "        'type_t': 'GRU'\n",
    "    }\n",
    "    wandb.init(config=config_defaults) # Initialize a new wandb run\n",
    "    config = wandb.config # config saves hyperparameters and inputs\n",
    "    encoder = Encoder(config.type_t, inp_lang.n_letters, config.emb_size, config.Target_tensor, config.dropout, config.hid_layers).to(device)\n",
    "    decoder = Att_Decoder(config.type_t, out_lang.n_letters, config.Target_tensor, config.dropout, config.hid_layers).to(device)\n",
    "    train = Train(train_pairs, encoder, decoder, nn.NLLLoss())\n",
    "    train.train_Iters(config.optimizer, config.learning_rate,print_every= 1000, epochs=config.epochs)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='assignment-3-attention')\n",
    "wandb.agent(sweep_id, function=run, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PxouD3BKY4W"
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs,test_pairs, inp_lang, out_lang = readLang('eng', 'hin')\n",
    "\n",
    "encoder = Encoder('GRU', inp_lang.n_letters, 512, 512, 0, 1).to(device)\n",
    "decoder = Att_Decoder('GRU', out_lang.n_letters, 512, 0, 1).to(device)\n",
    "train = Train(train_pairs, encoder, decoder, nn.NLLLoss())\n",
    "train.train_Iters('SGD', 0.01, print_every=1000, epochs=10)\n",
    "data = test_pairs\n",
    "Test_accuracy = train.evaluateData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_predictions(predictions, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Input', 'Output', 'Prediction'])\n",
    "        writer.writerows(predictions)\n",
    "\n",
    "# After training\n",
    "test_predictions = []\n",
    "\n",
    "for word, target in test_pairs:\n",
    "    output_word, _ = train.evaluate(word)\n",
    "    test_predictions.append([word, target, output_word])\n",
    "\n",
    "save_predictions(test_predictions, 'test_predictions2.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
